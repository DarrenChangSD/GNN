{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Week 5 Notebook: Building a Deep Learning Model\n",
    "===============================================================\n",
    "\n",
    "Now, we'll look at a deep learning model based on low-level track features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import uproot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 48 track-level features\n",
    "features = ['track_pt',\n",
    "            'track_ptrel',\n",
    "            'trackBTag_Eta',\n",
    "            'trackBTag_DeltaR',\n",
    "            'trackBTag_EtaRel',\n",
    "            'trackBTag_JetDistVal',\n",
    "            'trackBTag_Momentum',\n",
    "            'trackBTag_PPar',\n",
    "            'trackBTag_PParRatio',\n",
    "            'trackBTag_PtRatio',\n",
    "            'trackBTag_PtRel',\n",
    "            'trackBTag_Sip2dSig',\n",
    "            'trackBTag_Sip2dVal',\n",
    "            'trackBTag_Sip3dSig',\n",
    "            'trackBTag_Sip3dVal',\n",
    "            'track_VTX_ass',\n",
    "            'track_charge',\n",
    "            'track_deltaR',\n",
    "            'track_detadeta',\n",
    "            'track_dlambdadz',\n",
    "            'track_dlambdadz',\n",
    "            'track_dphidphi',\n",
    "            'track_dphidxy',\n",
    "            'track_dptdpt',\n",
    "            'track_drminsv',\n",
    "            'track_drsubjet1',\n",
    "            'track_drsubjet2',\n",
    "            'track_dxy',\n",
    "            'track_dxydxy',\n",
    "            'track_dxydz',\n",
    "            'track_dxysig',\n",
    "            'track_dz',\n",
    "            'track_dzdz',        \n",
    "            'track_dzsig',\n",
    "            'track_erel',\n",
    "            'track_etarel',\n",
    "            'track_fromPV',\n",
    "            'track_isChargedHad',\n",
    "            'track_isEl',\n",
    "            'track_isMu',\n",
    "            'track_lostInnerHits',\n",
    "            'track_mass',\n",
    "            'track_normchi2',            \n",
    "            'track_phirel',\n",
    "            'track_pt',\n",
    "            'track_ptrel',\n",
    "            'track_puppiw',\n",
    "            'track_quality']\n",
    "\n",
    "# spectators to define mass/pT window\n",
    "spectators = ['fj_sdmass',\n",
    "              'fj_pt']\n",
    "\n",
    "# 2 labels: QCD or Hbb (we'll reduce the following labels)\n",
    "labels =  ['label_QCD_b',\n",
    "           'label_QCD_bb',\n",
    "           'label_QCD_c', \n",
    "           'label_QCD_cc', \n",
    "           'label_QCD_others',\n",
    "           'sample_isQCD',\n",
    "           'label_H_bb']\n",
    "\n",
    "nfeatures = len(features)\n",
    "nspectators = len(spectators)\n",
    "nlabels = 2\n",
    "\n",
    "# we're going to zero-pad up to 60 tracks\n",
    "ntracks = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generators\n",
    "\n",
    "A quick aside on data generators. As training on large datasets is a key component of many deep learning approaches (and especially in high energy physics), and these datasets no longer fit in memory, it is imporatant to write a data generator which can automatically fetch data.\n",
    "\n",
    "Here we modify one from: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_files, features, labels, spectators, batch_size=8192, n_dim=60, \n",
    "                 remove_mass_pt_window=False, remove_unlabeled=True):\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_files = list_files\n",
    "        self.features = features\n",
    "        self.spectators = spectators\n",
    "        self.n_dim = n_dim\n",
    "        self.n_channels = len(features)\n",
    "        self.remove_mass_pt_window = remove_mass_pt_window\n",
    "        self.remove_unlabeled = remove_unlabeled\n",
    "        self.global_IDs = []\n",
    "        self.file_mapping = []\n",
    "        self.open_files = [None]*len(self.list_files)\n",
    "        running_total = 0\n",
    "        for i, file_name in enumerate(self.list_files):\n",
    "            root_file = uproot.open(file_name)\n",
    "            self.open_files.append(root_file)\n",
    "            tree = root_file['deepntuplizer/tree']\n",
    "            tree_length = len(tree)\n",
    "            self.global_IDs.append(np.arange(running_total+running_total+tree_length))\n",
    "            self.file_mapping.append(np.repeat(i,tree_length))\n",
    "            running_total += tree_length\n",
    "            root_file.close()\n",
    "        self.global_IDs = np.concatenate(self.global_IDs)\n",
    "        self.file_mapping = np.concatenate(self.file_mapping)\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.global_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        files = self.file_mapping[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        \n",
    "        unique_files = np.unique(files)\n",
    "        starts = np.array([min(indexes[files==i]) for i in unique_files])\n",
    "        stops = np.array([max(indexes[files==i]) for i in unique_files])\n",
    "\n",
    "        # Check if files needed open (if not open them)\n",
    "        # Also if file is not needed, close it\n",
    "        for ifile, file_name in enumerate(self.list_files):\n",
    "            if ifile in unique_files:\n",
    "                if self.open_files[ifile] is None: \n",
    "                    self.open_files[ifile] = uproot.open(file_name)\n",
    "            else:\n",
    "                if self.open_files[ifile] is not None: \n",
    "                    self.open_files[ifile].close()\n",
    "                    self.open_files[ifile] = None\n",
    "            \n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(unique_files, starts, stops)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = self.global_IDs\n",
    "\n",
    "    def __data_generation(self, unique_files, starts, stops):\n",
    "        'Generates data containing batch_size samples' \n",
    "        # X : (n_samples, n_dim, n_channels)\n",
    "        # y : (n_samples, 2)\n",
    "        Xs = []\n",
    "        ys = []\n",
    "        \n",
    "        # Generate data\n",
    "        for ifile, start, stop in zip(unique_files, starts, stops):\n",
    "            X, y = self.__get_features_labels(ifile, start, stop)\n",
    "            Xs.append(X)\n",
    "            ys.append(y)\n",
    "            \n",
    "        # Stack data if going over multiple files\n",
    "        if len(unique_files)>1:\n",
    "            X = np.stack(Xs,axis=0)\n",
    "            y = np.stack(ys,axis=0)\n",
    "            \n",
    "            \n",
    "        return X, y\n",
    "                         \n",
    "    def __get_features_labels(self, ifile, entrystart, entrystop):\n",
    "        'Loads data from one file'\n",
    "        \n",
    "        # Double check that file is open\n",
    "        if self.open_files[ifile] is None:\n",
    "            root_file = uproot.open(self.list_file[ifile])\n",
    "        else:\n",
    "            root_file = self.open_files[ifile]\n",
    "            \n",
    "        tree = root_file['deepntuplizer/tree']\n",
    "        \n",
    "        feature_array = tree.arrays(branches=self.features, \n",
    "                                    entrystart=entrystart,\n",
    "                                    entrystop=entrystop,\n",
    "                                    namedecode='utf-8')\n",
    "\n",
    "        label_array_all = tree.arrays(branches=self.labels, \n",
    "                                      entrystart=entrystart,\n",
    "                                      entrystop=entrystop,\n",
    "                                      namedecode='utf-8')\n",
    "\n",
    "        X = np.stack([feature_array[feat].pad(self.n_dim, clip=True).fillna(0).regular() for feat in features],axis=2)\n",
    "        n_samples = X.shape[0]\n",
    "    \n",
    "        y = np.zeros((n_samples,2))\n",
    "        y[:,0] = label_array_all['sample_isQCD'] * (label_array_all['label_QCD_b'] + \\\n",
    "                                                    label_array_all['label_QCD_bb'] + \\\n",
    "                                                    label_array_all['label_QCD_c'] + \\\n",
    "                                                    label_array_all['label_QCD_cc'] + \\\n",
    "                                                    label_array_all['label_QCD_others'])\n",
    "        y[:,1] = label_array_all['label_H_bb']\n",
    "\n",
    "        \n",
    "        if self.remove_mass_pt_window:\n",
    "            # remove data outside of mass/pT range\n",
    "            spec_array = tree.arrays(branches=self.spectators, \n",
    "                                     entrystart=entrystart,\n",
    "                                     entrystop=entrystop,\n",
    "                                     namedecode='utf-8')\n",
    "            \n",
    "            z = np.stack([spec_array[spec] for spec in self.spectators],axis=1)\n",
    "            X = X[(z[:,0] > 40) & (z[:,0] < 200) & (z[:,1] > 300) & (z[:,1] < 2000)]\n",
    "            y = y[(z[:,0] > 40) & (z[:,0] < 200) & (z[:,1] > 300) & (z[:,1] < 2000)]\n",
    "            z = z[(z[:,0] > 40) & (z[:,0] < 200) & (z[:,1] > 300) & (z[:,1] < 2000)]\n",
    "                        \n",
    "        if self.remove_unlabeled:\n",
    "            # remove unlabeled data\n",
    "            X = X[np.sum(y,axis=1)==1]\n",
    "            y = y[np.sum(y,axis=1)==1]\n",
    "            \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training and validation generators \n",
    "train_files = ['root://eospublic.cern.ch//eos/opendata/cms/datascience/HiggsToBBNtupleProducerTool/HiggsToBBNTuple_HiggsToBB_QCD_RunII_13TeV_MC/train/ntuple_merged_10.root']\n",
    "val_files = ['root://eospublic.cern.ch//eos/opendata/cms/datascience/HiggsToBBNtupleProducerTool/HiggsToBBNTuple_HiggsToBB_QCD_RunII_13TeV_MC/train/ntuple_merged_11.root']\n",
    "\n",
    "\n",
    "train_generator = DataGenerator(train_files, features, labels, spectators, batch_size=8192, n_dim=ntracks, \n",
    "                                remove_mass_pt_window=False, \n",
    "                                remove_unlabeled=True)\n",
    "\n",
    "val_generator = DataGenerator(val_files, features, labels, spectators, batch_size=8192, n_dim=ntracks, \n",
    "                                remove_mass_pt_window=False, \n",
    "                                remove_unlabeled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data Generator\n",
    "Note that the track array has a different \"shape.\" There are also slightly less than the requested `batch_size=8192` because we remove unlabeled samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7672, 60, 48)\n",
      "(7672, 2)\n"
     ]
    }
   ],
   "source": [
    "X, y = train_generator[0]\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note this generator can be optimized further (storing the data file locally, etc.). It's important to note that I/O is often a bottleneck for training big networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Neural Network Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 60, 48)            0         \n",
      "_________________________________________________________________\n",
      "bn_1 (BatchNormalization)    (None, 60, 48)            192       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2880)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                184384    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 187,778\n",
      "Trainable params: 187,682\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, BatchNormalization, Conv1D, Flatten, Lambda\n",
    "import keras.backend as K\n",
    "\n",
    "# define dense keras model\n",
    "inputs = Input(shape=(ntracks,nfeatures,), name = 'input')  \n",
    "x = BatchNormalization(name='bn_1')(inputs)\n",
    "x = Flatten(name='flatten_1')(x)\n",
    "x = Dense(64, name = 'dense_1', activation='relu')(x)\n",
    "x = Dense(32, name = 'dense_2', activation='relu')(x)\n",
    "x = Dense(32, name = 'dense_3', activation='relu')(x)\n",
    "outputs = Dense(nlabels, name = 'output', activation='softmax')(x)\n",
    "keras_model_dense = Model(inputs=inputs, outputs=outputs)\n",
    "keras_model_dense.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(keras_model_dense.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "23/24 [===========================>..] - ETA: 6s - loss: 0.3622 - accuracy: 0.8636 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/utils/data_utils.py:616: UserWarning: The input 11 could not be retrieved. It could be because a worker has died.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "24/24 [==============================] - 325s 14s/step - loss: 0.3590 - accuracy: 0.8643 - val_loss: 0.3547 - val_accuracy: 0.8692\n"
     ]
    }
   ],
   "source": [
    "# define callbacks\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20)\n",
    "reduce_lr = ReduceLROnPlateau(patience=5,factor=0.5)\n",
    "model_checkpoint = ModelCheckpoint('keras_model_dense_best.h5', monitor='val_loss', save_best_only=True)\n",
    "callbacks = [early_stopping, model_checkpoint, reduce_lr]\n",
    "\n",
    "# fit keras model\n",
    "history_dense = keras_model_dense.fit_generator(train_generator, \n",
    "                                                validation_data = val_generator, \n",
    "                                                steps_per_epoch=len(train_generator), \n",
    "                                                validation_steps=len(val_generator),\n",
    "                                                max_queue_size=5,\n",
    "                                                epochs=1, \n",
    "                                                shuffle=False,\n",
    "                                                callbacks = callbacks, \n",
    "                                                verbose=1)\n",
    "# reload best weights\n",
    "keras_model_dense.load_weights('keras_model_dense_best.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(history_dense.history['loss'],label='Loss')\n",
    "plt.plot(history_dense.history['val_loss'],label='Val. loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional 1D Neural Network Classifier\n",
    "\n",
    "This model uses the `Conv1D` layer of Keras, but really it's more like the Deep Sets architecture applied to jets, the so-caled Particle-flow network approach{cite}`Komiske:2018cqr,NIPS2017_6931`.\n",
    "By using a kernel size of 1, we are applying the same fully connected neural network to each track. \n",
    "Then the `Lambda` layer sums over the tracks (actually it takes the mean). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 60, 48)            0         \n",
      "_________________________________________________________________\n",
      "bn_1 (BatchNormalization)    (None, 60, 48)            192       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 60, 64)            3136      \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 60, 32)            2080      \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 60, 32)            1056      \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               3300      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 9,966\n",
      "Trainable params: 9,870\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, BatchNormalization, Conv1D, Flatten, Lambda\n",
    "import keras.backend as K\n",
    "\n",
    "# define conv1d keras model\n",
    "inputs = Input(shape=(ntracks,nfeatures,), name = 'input')  \n",
    "x = BatchNormalization(name='bn_1')(inputs)\n",
    "x = Conv1D(64, 1, strides=1, padding='same', name = 'conv1d_1', activation='relu')(x)\n",
    "x = Conv1D(32, 1, strides=1, padding='same', name = 'conv1d_2', activation='relu')(x)\n",
    "x = Conv1D(32, 1, strides=1, padding='same', name = 'conv1d_3', activation='relu')(x)\n",
    "# sum over tracks\n",
    "x = Lambda(lambda x: K.mean(x, axis=-2), name = 'lambda_1', input_shape=(ntracks,32))(x)\n",
    "x = Dense(100, name = 'dense_1', activation='relu')(x)\n",
    "outputs = Dense(nlabels, name = 'output', activation='softmax')(x)\n",
    "keras_model_conv1d = Model(inputs=inputs, outputs=outputs)\n",
    "keras_model_conv1d.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(keras_model_conv1d.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "24/24 [==============================] - 314s 13s/step - loss: 0.5101 - accuracy: 0.7945 - val_loss: 0.3984 - val_accuracy: 0.8682\n"
     ]
    }
   ],
   "source": [
    "# define callbacks\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20)\n",
    "reduce_lr = ReduceLROnPlateau(patience=5,factor=0.5)\n",
    "model_checkpoint = ModelCheckpoint('keras_model_conv1d_best.h5', monitor='val_loss', save_best_only=True)\n",
    "callbacks = [early_stopping, model_checkpoint, reduce_lr]\n",
    "\n",
    "# fit keras model\n",
    "history_conv1d = keras_model_conv1d.fit(train_generator, \n",
    "                                        validation_data = val_generator, \n",
    "                                        steps_per_epoch=len(train_generator), \n",
    "                                        validation_steps=len(val_generator),\n",
    "                                        max_queue_size=5,\n",
    "                                        epochs=1, \n",
    "                                        shuffle=False,\n",
    "                                        callbacks = callbacks, \n",
    "                                        verbose=1)\n",
    "# reload best weights\n",
    "keras_model_conv1d.load_weights('keras_model_conv1d_best.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEGCAYAAABrQF4qAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAVgUlEQVR4nO3df7DddZ3f8eeLBAhKCgHiLxKa0GUHE4iAx0ibWkCrQh0TWaxLViusdSh0qYxoSxyr/CozQqnLgMx0qLMOo20jteNMnMimaEV0u2AuLMIGZA2BLRfUDQGi2TSGH+/+cU/Yk8tN7knuTU7yyfMxc+ae7+fz+X7P+3Mz87qffL/nfE+qCklSuw4adAGSpD3LoJekxhn0ktQ4g16SGmfQS1Ljpg66gNGOOeaYmjNnzqDLkKT9yv333/9sVc0cq2+fC/o5c+YwNDQ06DIkab+S5K931OepG0lqnEEvSY0z6CWpcfvcOXpJB5YXX3yR4eFhtmzZMuhS9gvTpk1j1qxZHHzwwX3vY9BLGqjh4WGmT5/OnDlzSDLocvZpVcWGDRsYHh5m7ty5fe/nqRtJA7VlyxaOPvpoQ74PSTj66KN3+X8/Br2kgTPk+7c7vyuDXpIaZ9BLOuAdfvjhgy5hjzLoJalxBr0kjeHBBx/k9NNPZ8GCBZx77rk8//zzANx8883MmzePBQsWcP755wPwwx/+kFNOOYVTTjmFU089ld/85jeDLP01fHulpH3G1d9ZwyPP/HpSjznvLX+PKz84f5f3+/jHP84tt9zCGWecwRe/+EWuvvpqbrrpJr70pS/xxBNPcOihh/LCCy8AcOONN3LrrbeyaNEiNm3axLRp0yZ1DhPlil6SRtm4cSMvvPACZ5xxBgAXXHAB99xzDwALFizgox/9KN/4xjeYOnVkrbxo0SIuv/xybr75Zl544YVX2/cV+1Y1kg5ou7Py3ttWrlzJPffcw3e+8x2uu+46Hn74YZYtW8YHPvABvvvd77Jo0SJWrVrFiSeeOOhSX+WKXpJGOeKII5gxYwY/+tGPAPj617/OGWecwSuvvMJTTz3FWWedxfXXX8/GjRvZtGkTjz/+OCeffDJXXHEF73jHO/jZz3424BlszxW9pAPe5s2bmTVr1qvbl19+ObfffjsXX3wxmzdv5vjjj+drX/saL7/8Mh/72MfYuHEjVcWnPvUpjjzySL7whS/wgx/8gIMOOoj58+dzzjnnDHA2r2XQSzrgvfLKK2O233vvva9p+/GPf/yatltuuWXSa5pMnrqRpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9Jjesr6JOcneSxJGuTLBuj/8Ik65M82H18sqfvgiQ/7z4umMziJWmizjrrLFatWrVd20033cQll1yyw33OPPNMhoaGdnrcq666ihtvvHFSapyocYM+yRTgVuAcYB6wNMm8MYZ+s6pO6T6+2t33KOBK4J3AQuDKJDMmrXpJmqClS5eyfPny7dqWL1/O0qVLB1TR5OtnRb8QWFtV66pqK7AcWNLn8d8P3FVVz1XV88BdwNm7V6okTb4Pf/jDrFy5kq1btwLw5JNP8swzz/Cud72LSy65hE6nw/z587nyyit3+zUGfcvjfj4ZeyzwVM/2MCMr9NHOS/JPgL8CPl1VT+1g32NH75jkIuAigOOOO66/yiW1585l8MuHJ/eYbzoZzvnSDruPOuooFi5cyJ133smSJUtYvnw5H/nIR0jCddddx1FHHcXLL7/Me97zHh566CEWLFiwyyUM+pbHk3Ux9jvAnKpawMiq/fZd2bmqbquqTlV1Zs6cOUklSVJ/ek/f9J62ueOOOzjttNM49dRTWbNmDY888sguH3tfuOVxP0d4Gpjdsz2r2/aqqtrQs/lV4Iaefc8cte/du1qkpAPETlbee9KSJUv49Kc/zQMPPMDmzZt5+9vfzhNPPMGNN97I6tWrmTFjBhdeeCFbtmyZ1NfdW7c87mdFvxo4IcncJIcA5wMregckeXPP5mLg0e7zVcD7kszoXoR9X7dNkvYZhx9+OGeddRaf+MQnXl3N//rXv+b1r389RxxxBL/61a+48847d+vY+8Itj8dd0VfVS0kuZSSgpwB/UlVrklwDDFXVCuBTSRYDLwHPARd2930uybWM/LEAuKaqnptw1ZI0yZYuXcq555776imct73tbZx66qmceOKJzJ49m0WLFo253yc/+UkuvvhiOp3ODo896Fsep6omfJDJ1Ol0arz3p0pqx6OPPspb3/rWQZexXxnrd5bk/qoa86+Nn4yVpMYZ9JLUOINe0sDta6eQ92W787sy6CUN1LRp09iwYYNh34eqYsOGDbv8ISq/M1bSQM2aNYvh4WHWr18/6FL2C9OmTdvui8z7YdBLGqiDDz6YuXPnDrqMpnnqRpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9Jjesr6JOcneSxJGuTLNvJuPOSVJJOd/vgJLcneTjJo0k+N1mFS5L6M27QJ5kC3AqcA8wDliaZN8a46cBlwH09zf8cOLSqTgbeDvyrJHMmoW5JUp/6WdEvBNZW1bqq2gosB5aMMe5a4HpgS09bAa9PMhU4DNgK/HpiJUuSdkU/QX8s8FTP9nC37VVJTgNmV9XKUft+C/hb4BfA/wVurKrnRr9AkouSDCUZWr9+/a7UL0kax4QvxiY5CPgy8JkxuhcCLwNvAeYCn0ly/OhBVXVbVXWqqjNz5syJliRJ6jG1jzFPA7N7tmd127aZDpwE3J0E4E3AiiSLgT8A/rSqXgT+JsmfAR1g3STULknqQz8r+tXACUnmJjkEOB9Ysa2zqjZW1TFVNaeq5gD3AouraoiR0zXvBkjyeuB04GeTPAdJ0k6MG/RV9RJwKbAKeBS4o6rWJLmmu2rfmVuBw5OsYeQPxteq6qGJFi1J6l+qatA1bKfT6dTQ0NCgy5Ck/UqS+6uqM1afn4yVpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhrXV9AnOTvJY0nWJlm2k3HnJakknZ62BUn+PMmaJA8nmTYZhUuS+jN1vAFJpgC3Au8FhoHVSVZU1SOjxk0HLgPu62mbCnwD+BdV9dMkRwMvTmL9kqRx9LOiXwisrap1VbUVWA4sGWPctcD1wJaetvcBD1XVTwGqakNVvTzBmiVJu6CfoD8WeKpne7jb9qokpwGzq2rlqH1/F6gkq5I8kOTfjfUCSS5KMpRkaP369btQviRpPBO+GJvkIODLwGfG6J4K/GPgo92f5yZ5z+hBVXVbVXWqqjNz5syJliRJ6tFP0D8NzO7ZntVt22Y6cBJwd5IngdOBFd0LssPAPVX1bFVtBr4LnDYZhUuS+tNP0K8GTkgyN8khwPnAim2dVbWxqo6pqjlVNQe4F1hcVUPAKuDkJK/rXpg9A3jktS8hSdpTxg36qnoJuJSR0H4UuKOq1iS5JsnicfZ9npHTOquBB4EHxjiPL0nag1JVg65hO51Op4aGhgZdhiTtV5LcX1Wdsfr8ZKwkNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TG9RX0Sc5O8liStUmW7WTceUkqSWdU+3FJNiX57EQLliTtmnGDPskU4FbgHGAesDTJvDHGTQcuA+4b4zBfBu6cWKmSpN3Rz4p+IbC2qtZV1VZgObBkjHHXAtcDW3obk3wIeAJYM7FSJUm7o5+gPxZ4qmd7uNv2qiSnAbOrauWo9sOBK4Crd/YCSS5KMpRkaP369X0VLknqz4QvxiY5iJFTM58Zo/sq4I+ratPOjlFVt1VVp6o6M2fOnGhJkqQeU/sY8zQwu2d7Vrdtm+nAScDdSQDeBKxIshh4J/DhJDcARwKvJNlSVV+ZhNolSX3oJ+hXAyckmctIwJ8P/MG2zqraCByzbTvJ3cBnq2oIeFdP+1XAJkNekvaucU/dVNVLwKXAKuBR4I6qWpPkmu6qXZK0D0tVDbqG7XQ6nRoaGhp0GZK0X0lyf1V1xurzk7GS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIa11fQJzk7yWNJ1iZZtpNx5yWpJJ3u9nuT3J/k4e7Pd09W4ZKk/kwdb0CSKcCtwHuBYWB1khVV9ciocdOBy4D7epqfBT5YVc8kOQlYBRw7WcVLksbXz4p+IbC2qtZV1VZgObBkjHHXAtcDW7Y1VNVfVNUz3c01wGFJDp1gzZKkXdBP0B8LPNWzPcyoVXmS04DZVbVyJ8c5D3igqn67y1VKknbbuKduxpPkIODLwIU7GTOfkdX++3bQfxFwEcBxxx030ZIkST36WdE/Dczu2Z7VbdtmOnAScHeSJ4HTgRU9F2RnAd8GPl5Vj4/1AlV1W1V1qqozc+bMXZ+FJGmH+gn61cAJSeYmOQQ4H1ixrbOqNlbVMVU1p6rmAPcCi6tqKMmRwEpgWVX92eSXL0kaz7hBX1UvAZcy8o6ZR4E7qmpNkmuSLB5n90uB3wG+mOTB7uMNE65aktS3VNWga9hOp9OpoaGhQZchSfuVJPdXVWesPj8ZK0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1rq+gT3J2kseSrE2ybCfjzktSSTo9bZ/r7vdYkvdPRtGSpP5NHW9AkinArcB7gWFgdZIVVfXIqHHTgcuA+3ra5gHnA/OBtwDfS/K7VfXy5E1BkrQz/azoFwJrq2pdVW0FlgNLxhh3LXA9sKWnbQmwvKp+W1VPAGu7x5Mk7SX9BP2xwFM928PdtlclOQ2YXVUrd3Xf7v4XJRlKMrR+/fq+Cpck9WfCF2OTHAR8GfjM7h6jqm6rqk5VdWbOnDnRkiRJPcY9Rw88Dczu2Z7VbdtmOnAScHcSgDcBK5Is7mNfSdIe1s+KfjVwQpK5SQ5h5OLqim2dVbWxqo6pqjlVNQe4F1hcVUPdcecnOTTJXOAE4CeTPgtJ0g6Nu6KvqpeSXAqsAqYAf1JVa5JcAwxV1Yqd7LsmyR3AI8BLwB/5jhtJ2rtSVYOuYTudTqeGhoYGXYYk7VeS3F9VnbH6/GSsJDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mN2+feR59kPfDXg65jNxwDPDvoIvYy53xgONDmvL/O9+9X1Zg3C9vngn5/lWRoRx9WaJVzPjAcaHNucb6eupGkxhn0ktQ4g37y3DboAgbAOR8YDrQ5Nzdfz9FLUuNc0UtS4wx6SWqcQb8LkhyV5K4kP+/+nLGDcRd0x/w8yQVj9K9I8pd7vuKJm8ick7wuycokP0uyJsmX9m71/UtydpLHkqxNsmyM/kOTfLPbf1+SOT19n+u2P5bk/Xu18AnY3TkneW+S+5M83P357r1e/G6ayL9zt/+4JJuSfHavFT0ZqspHnw/gBmBZ9/ky4PoxxhwFrOv+nNF9PqOn//eA/wb85aDns6fnDLwOOKs75hDgR8A5g57TGPVPAR4Hju/W+VNg3qgx/xr4z93n5wPf7D6f1x1/KDC3e5wpg57THp7zqcBbus9PAp4e9Hz29Jx7+r8F/A/gs4Oez648XNHvmiXA7d3ntwMfGmPM+4G7quq5qnoeuAs4GyDJ4cDlwH/Y86VOmt2ec1VtrqofAFTVVuABRr4gfl+zEFhbVeu6dS5nZN69en8P3wLekyTd9uVV9duqegJY2z3evm6351xVf1FVz3Tb1wCHJTl0r1Q9MRP5dybJh4AnGJnzfsWg3zVvrKpfdJ//EnjjGGOOBZ7q2R7utgFcC/wnYPMeq3DyTXTOACQ5Evgg8P09UONEjVt/75iqegnYCBzd5777oonMudd5wANV9ds9VOdk2u05dxdpVwBX74U6J924Xw5+oEnyPeBNY3R9vnejqipJ3+9NTXIK8A+q6tOjz/sN2p6ac8/xpwL/Hbi5qtbtXpXa1ySZD1wPvG/QtewFVwF/XFWbugv8/YpBP0pV/dMd9SX5VZI3V9UvkrwZ+Jsxhj0NnNmzPQu4G/iHQCfJk4z83t+Q5O6qOpMB24Nz3uY24OdVddPEq90jngZm92zP6raNNWa4+4frCGBDn/vuiyYyZ5LMAr4NfLyqHt/z5U6Kicz5ncCHk9wAHAm8kmRLVX1lj1c9GQZ9kWB/egD/ke0vTN4wxpijGDmPN6P7eAI4atSYOew/F2MnNGdGrkf8T+CgQc9lJ3OcysgF5Ln83UW6+aPG/BHbX6S7o/t8PttfjF3H/nExdiJzPrI7/vcGPY+9NedRY65iP7sYO/AC9qcHI+cnvw/8HPheT5h1gK/2jPsEIxfl1gJ/OMZx9qeg3+05M7JiKuBR4MHu45ODntMO5vnPgL9i5F0Zn++2XQMs7j6fxsi7LdYCPwGO79n38939HmMffFfRZM8Z+PfA3/b8mz4IvGHQ89nT/849x9jvgt5bIEhS43zXjSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6HZCSvJzkwZ7Ha+5kOIFjz9lf7k6qA4OfjNWB6v9V1SmDLkLaG1zRSz2SPJnkhu691n+S5He67XOS/O8kDyX5fpLjuu1vTPLtJD/tPv5R91BTkvyX7n34/1eSwwY2KR3wDHodqA4bderm93v6NlbVycBXgJu6bbcAt1fVAuC/Ajd3228GflhVbwNO4+9uYXsCcGtVzQdeYOQuj9JA+MlYHZCSbKqqw8dofxJ4d1WtS3Iw8MuqOjrJs8Cbq+rFbvsvquqYJOuBWdVzm97u3UnvqqoTuttXAAdX1f70PQRqiCt66bVqB893Re/92V/G62EaIINeeq3f7/n5593n/4eRuxkCfJSRr0WEkRu+XQKQZEqSI/ZWkVK/XGXoQHVYkgd7tv+0qra9xXJGkocYWZUv7bb9G+BrSf4tsB74w277ZcBtSf4lIyv3S4BfIO1DPEcv9eieo+9U1bODrkWaLJ66kaTGuaKXpMa5opekxhn0ktQ4g16SGmfQS1LjDHpJatz/BwAljBNU2QY5AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(history_conv1d.history['loss'],label='Loss')\n",
    "plt.plot(history_conv1d.history['val_loss'],label='Val. loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load testing file\n",
    "test_files = ['root://eospublic.cern.ch//eos/opendata/cms/datascience/HiggsToBBNtupleProducerTool/HiggsToBBNTuple_HiggsToBB_QCD_RunII_13TeV_MC/test/ntuple_merged_0.root']\n",
    "test_generator = DataGenerator(test_files, features, labels, spectators, batch_size=8192, n_dim=ntracks, \n",
    "                               remove_mass_pt_window=True, \n",
    "                               remove_unlabeled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all input arrays must have the same shape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-8c0217961178>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mpredict_array_cnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_model_conv1d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mpredict_array_dnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_array_dnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mpredict_array_cnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_array_cnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mlabel_array_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_array_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all input arrays must have the same shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0mresult_ndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all input arrays must have the same shape"
     ]
    }
   ],
   "source": [
    "# run model inference on test data set\n",
    "predict_array_dnn = []\n",
    "predict_array_cnn = []\n",
    "label_array_test = []\n",
    "\n",
    "for t in test_generator:\n",
    "    label_array_test.append(t[1])\n",
    "    predict_array_dnn.append(keras_model_dense.predict(t[0]))\n",
    "    predict_array_cnn.append(keras_model_conv1d.predict(t[0]))\n",
    "    \n",
    "predict_array_dnn = np.stack(predict_array_dnn,axis=0)\n",
    "predict_array_cnn = np.stack(predict_array_cnn,axis=0)\n",
    "label_array_test = np.stack(label_array_test,axis=0)\n",
    "\n",
    "# create ROC curves\n",
    "fpr_dnn, tpr_dnn, threshold_dnn = roc_curve(label_array_test[:,1], predict_array_dnn[:,1])\n",
    "fpr_cnn, tpr_cnn, threshold_cnn = roc_curve(label_array_test[:,1], predict_array_cnn[:,1])\n",
    "    \n",
    "# plot ROC curves\n",
    "plt.figure()\n",
    "plt.plot(tpr_dnn, fpr_dnn, lw=2.5, label=\"Dense, AUC = {:.1f}%\".format(auc(fpr_dnn,tpr_dnn)*100))\n",
    "plt.plot(tpr_cnn, fpr_cnn, lw=2.5, label=\"Conv1D, AUC = {:.1f}%\".format(auc(fpr_cnn,tpr_cnn)*100))\n",
    "plt.xlabel(r'True positive rate')\n",
    "plt.ylabel(r'False positive rate')\n",
    "plt.semilogy()\n",
    "plt.ylim(0.001,1)\n",
    "plt.xlim(0,1)\n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the more structurally-aware Conv1D/DeepSets model does better than a simple fully conneted neural network appraoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
